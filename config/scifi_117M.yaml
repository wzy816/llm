# model
dim: 768
num_layers: 12
num_heads: 12

head_dim: 128
max_seq_len: 1024

# dataset
context_size: 1024
sample_every: 256

# train
batch_size: 32
micro_batch_size: 100
min_save_step: 20
max_save_loss: 2.0
max_train_step: 2000
min_train_lr: 1.e-12

# optimizer
lr: 3.e-4
weight_decay:  0.1

# ReduceLROnPlateau
patience:  50
factor: 0.1

# placeholder
total_params: ''
vocab_size: 0