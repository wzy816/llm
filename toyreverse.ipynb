{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d15df186",
   "metadata": {},
   "source": [
    "# toy reverse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0bc90e",
   "metadata": {},
   "source": [
    "## 1. generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e34b406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "def generate_line():\n",
    "    pool = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'\n",
    "    return ''.join([random.choice(pool) for i in range(32)])\n",
    "\n",
    "\n",
    "data_dir = '/mnt/llm/data/toyreverse'\n",
    "\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "with open(os.path.join(data_dir, 'train.txt'), 'w') as f:\n",
    "    for _ in range(1000000):\n",
    "        line = generate_line()\n",
    "        f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dba90fe",
   "metadata": {},
   "source": [
    "## 2. train tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b5042f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input_format: \n",
      "  model_prefix: /mnt/llm/tokenizer/toyreverse\n",
      "  model_type: BPE\n",
      "  vocab_size: 70\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 1\n",
      "  bos_id: 2\n",
      "  eos_id: 3\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ‚Åá \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(145) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(145) LOG(INFO) Loaded 2000000 lines\n",
      "trainer_interface.cc(122) LOG(WARNING) Too many sentences are loaded! (2000000), which may slow down training.\n",
      "trainer_interface.cc(124) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(127) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 2000000 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=66000000\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=63\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 2000000 sentences.\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 2000000\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 2000000\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=32758 min_freq=16182\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: /mnt/llm/tokenizer/toyreverse.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: /mnt/llm/tokenizer/toyreverse.vocab\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import sentencepiece as spm\n",
    "import re\n",
    "\n",
    "\n",
    "def iter(directory):\n",
    "    filenames = [f for f in os.listdir(directory) if f.endswith('.txt')]\n",
    "    for fn in filenames:\n",
    "        with open(os.path.join(directory, fn), 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                line = line.strip()\n",
    "                yield line.encode()\n",
    "\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    sentence_iterator=iter('/mnt/llm/data/toyreverse'),\n",
    "    model_prefix='/mnt/llm/tokenizer/toyreverse',\n",
    "    model_type='bpe',\n",
    "    vocab_size=70,\n",
    "    pad_id=0,\n",
    "    unk_id=1,\n",
    "    bos_id=2,\n",
    "    eos_id=3,\n",
    "    user_defined_symbols=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b729fa90",
   "metadata": {},
   "source": [
    "## 3. train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "146fe020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.w = nn.Parameter(torch.ones(dim), requires_grad=False)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "        return self.w * x\n",
    "\n",
    "\n",
    "def apply_rotary_position_embedding(x, freqs):\n",
    "    x = x.float().reshape(*x.shape[:-1], -1, 2)\n",
    "    x = torch.view_as_complex(x)\n",
    "\n",
    "    _, seq_len, _, half_head_dim = x.shape\n",
    "    freqs = freqs[0:seq_len].view(1, seq_len, 1, half_head_dim)\n",
    "\n",
    "    o = torch.view_as_real(x * freqs)\n",
    "    return o.flatten(3)\n",
    "\n",
    "\n",
    "def compute_freqs(head_dim, max_seq_len):\n",
    "    freqs = 1.0 / (10000**(\n",
    "        torch.arange(0, head_dim, 2)[:(head_dim // 2)].float() / head_dim))\n",
    "    t = torch.arange(max_seq_len * 2)\n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    return torch.polar(torch.ones_like(freqs), freqs)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(4 * dim * 2 / 3)\n",
    "        hidden_dim = hidden_dim + 256 - hidden_dim % 256  # multiple of 256\n",
    "        self.w = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.v = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.w_2 = nn.Linear(hidden_dim, dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(F.silu(self.w(x)) * self.v(x))\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads, head_dim, freqs):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.freqs = freqs\n",
    "\n",
    "        self.wq = nn.Linear(dim, num_heads * head_dim, bias=False)\n",
    "        self.wk = nn.Linear(dim, num_heads * head_dim, bias=False)\n",
    "        self.wv = nn.Linear(dim, num_heads * head_dim, bias=False)\n",
    "        self.wo = nn.Linear(num_heads * head_dim, dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, dim = x.size()\n",
    "        q = self.wq(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        k = self.wk(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        v = self.wv(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        q = apply_rotary_position_embedding(q, self.freqs)\n",
    "        k = apply_rotary_position_embedding(k, self.freqs)\n",
    "\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        scores = torch.matmul(q, k.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        scores = F.softmax(scores.float(), dim=-1)\n",
    "\n",
    "        o = torch.matmul(scores, v)\n",
    "        o = o.transpose(1, 2).contiguous().view(batch_size, seq_len, dim)\n",
    "        o = self.wo(o)\n",
    "        return o\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads, head_dim, max_seq_len):\n",
    "        super().__init__()\n",
    "        self.norm_1 = RMSNorm(dim)\n",
    "        self.attention = Attention(dim, num_heads, head_dim, max_seq_len)\n",
    "        self.norm_2 = RMSNorm(dim)\n",
    "        self.mlp = MLP(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + checkpoint(self.custom(self.attention), self.norm_1(x))\n",
    "        x = x + checkpoint(self.custom(self.mlp), self.norm_2(x))\n",
    "        return x\n",
    "\n",
    "    def custom(self, module):\n",
    "\n",
    "        def custom_forward(*inputs):\n",
    "            inputs = module(inputs[0])\n",
    "            return inputs\n",
    "\n",
    "        return custom_forward\n",
    "\n",
    "\n",
    "class LLM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, padding_idx, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, config.dim, padding_idx)\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        freqs = compute_freqs(config.head_dim, config.max_seq_len)\n",
    "        for i in range(config.num_layers):\n",
    "            block = Block(config.dim, config.num_heads, config.head_dim, freqs)\n",
    "            self.layers.append(block)\n",
    "\n",
    "        self.norm = RMSNorm(config.dim)\n",
    "        self.output = nn.Linear(config.dim, vocab_size, bias=False)\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, module in self.named_modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                factor = 1\n",
    "                if name.endswith('wo'):\n",
    "                    factor = 1 / math.sqrt(2 * self.config.num_layers)\n",
    "                torch.nn.init.normal_(module.weight,\n",
    "                                      mean=0.0,\n",
    "                                      std=0.02 * factor)\n",
    "            if isinstance(module, nn.Embedding):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.embedding(x)\n",
    "        for layer in self.layers:\n",
    "            h = layer(h)\n",
    "        h = self.norm(h)\n",
    "        o = self.output(h)\n",
    "        return o.float()\n",
    "\n",
    "    def count_parameters_B(self):\n",
    "        total = 0\n",
    "        for name, module in self.named_modules():\n",
    "            if isinstance(module, nn.Linear) or isinstance(\n",
    "                    module, nn.Embedding):\n",
    "                total += sum([p.numel() for p in list(module.parameters())])\n",
    "        return total / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4a5637d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class ToyReserveDataset(torch.utils.data.IterableDataset):\n",
    "\n",
    "    def __init__(self, context_size, tokenizer, data_dir):\n",
    "        self.context_size = context_size\n",
    "        self.files = list(Path(data_dir).rglob('*.txt'))\n",
    "        assert len(self.files) > 0\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad_id = tokenizer.pad_id()\n",
    "\n",
    "    def __iter__(self):\n",
    "        for fn in self.files:\n",
    "            with open(fn) as f:\n",
    "                for line in f.readlines():\n",
    "                    if line.strip() == '':\n",
    "                        continue\n",
    "                    while len(\n",
    "                            self.tokenizer.encode(line)) >= self.context_size:\n",
    "                        line = line[:-2]\n",
    "\n",
    "                    x = torch.LongTensor(self.tokenizer.encode(line))\n",
    "                    x = F.pad(x, (0, self.context_size - x.shape[0]),\n",
    "                              \"constant\", self.pad_id)\n",
    "\n",
    "                    y = torch.LongTensor(self.tokenizer.encode(line[::-1]))\n",
    "                    y = F.pad(y, (self.context_size - y.shape[0], 0),\n",
    "                              \"constant\", self.pad_id)\n",
    "\n",
    "                    yield x.cuda(), y.cuda()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # model\n",
    "    dim: int = 2560\n",
    "    num_layers: int = 32\n",
    "    num_heads: int = 1\n",
    "    head_dim: int = dim // num_heads\n",
    "    max_seq_len: int = 32  # same as contenxt_size\n",
    "\n",
    "    # adamw\n",
    "    learning_rate: float = 1e-6\n",
    "    weight_decay: float = 0.01\n",
    "\n",
    "    # data\n",
    "    batch_size: int = 8\n",
    "    context_size: int = 32\n",
    "\n",
    "\n",
    "class Trainer():\n",
    "\n",
    "    def __init__(self, project, tokenizer, data_dir, output_dir):\n",
    "        self.project = project\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = self.tokenizer.vocab_size()\n",
    "        self.padding_idx = self.tokenizer.pad_id()\n",
    "\n",
    "        self.config = Config()\n",
    "        self.llm = LLM(self.vocab_size, self.padding_idx, self.config)\n",
    "        self.llm.init_weights()\n",
    "        print(self.llm)\n",
    "        print(f'model parameter {self.llm.count_parameters_B():.2f}B')\n",
    "\n",
    "        self.train_dataset = ToyReserveDataset(self.config.context_size,\n",
    "                                               self.tokenizer, data_dir)\n",
    "        self.train_loader = DataLoader(self.train_dataset,\n",
    "                                       batch_size=self.config.batch_size)\n",
    "\n",
    "        self.optimizer = AdamW(\n",
    "            self.llm.parameters(),\n",
    "            lr=self.config.learning_rate,\n",
    "            weight_decay=self.config.weight_decay,\n",
    "        )\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "    def compute_loss(self, logits, y):\n",
    "        return F.cross_entropy(logits.view(-1, self.vocab_size), y.view(-1))\n",
    "\n",
    "    def train(self):\n",
    "        wandb.init(project=self.project,\n",
    "                   name=datetime.datetime.now().strftime('%Y%m%d_%H%M%S'),\n",
    "                   config=self.config)\n",
    "        self.llm.train()\n",
    "\n",
    "        step = 0\n",
    "        token_cnt = 0\n",
    "        best_loss = math.inf\n",
    "        patience = 0\n",
    "        last_save_step = 0\n",
    "\n",
    "        for batch_idx, sample in enumerate(self.train_loader):\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            x, y = sample\n",
    "            token_cnt += int(torch.count_nonzero(x))\n",
    "            logits = self.llm.forward(x)\n",
    "\n",
    "            loss = self.compute_loss(logits, y)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            wandb.log({\n",
    "                'loss': loss,\n",
    "                'token_cnt': token_cnt,\n",
    "            }, step=step)\n",
    "            patience += 1\n",
    "\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                wandb.log({\"best_loss\": best_loss}, step=step)\n",
    "                patience = 0\n",
    "\n",
    "                if step - last_save_step > 100:\n",
    "                    self.save(step)\n",
    "                    last_save_step = step\n",
    "\n",
    "            if patience > 1000:\n",
    "                break\n",
    "            step += 1\n",
    "        wandb.finish()\n",
    "\n",
    "    def save(self, step):\n",
    "        directory = os.path.join(self.output_dir, f'step={step}')\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        torch.save(self.llm.state_dict(), directory + '/weights.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24ec7bf4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM(\n",
      "  (embedding): Embedding(70, 2560, padding_idx=0)\n",
      "  (layers): ModuleList(\n",
      "    (0-31): 32 x Block(\n",
      "      (norm_1): RMSNorm()\n",
      "      (attention): Attention(\n",
      "        (wq): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "        (wk): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "        (wv): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "        (wo): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "      )\n",
      "      (norm_2): RMSNorm()\n",
      "      (mlp): MLP(\n",
      "        (w): Linear(in_features=2560, out_features=6912, bias=False)\n",
      "        (v): Linear(in_features=2560, out_features=6912, bias=False)\n",
      "        (w_2): Linear(in_features=6912, out_features=2560, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): RMSNorm()\n",
      "  (output): Linear(in_features=2560, out_features=70, bias=False)\n",
      ")\n",
      "model parameter 2.54B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwzy816\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/llm/wandb/run-20230721_210525-o06rusv4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/wzy816/llm_toyreverse/runs/o06rusv4' target=\"_blank\">20230721_210523</a></strong> to <a href='https://wandb.ai/wzy816/llm_toyreverse' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/wzy816/llm_toyreverse' target=\"_blank\">https://wandb.ai/wzy816/llm_toyreverse</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/wzy816/llm_toyreverse/runs/o06rusv4' target=\"_blank\">https://wandb.ai/wzy816/llm_toyreverse/runs/o06rusv4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c28798c68014d2c9f447584dac0a948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.014 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.206254‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_loss</td><td>‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>loss</td><td>‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>lr</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>token_cnt</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_loss</td><td>0.00026</td></tr><tr><td>loss</td><td>0.00038</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>token_cnt</td><td>3524685</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">20230721_210523</strong> at: <a href='https://wandb.ai/wzy816/llm_toyreverse/runs/o06rusv4' target=\"_blank\">https://wandb.ai/wzy816/llm_toyreverse/runs/o06rusv4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230721_210525-o06rusv4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.set_default_device('cuda')\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "tokenizer = spm.SentencePieceProcessor('/mnt/llm/tokenizer/toyreverse.model')\n",
    "\n",
    "trainer = Trainer(project='llm_toyreverse',\n",
    "                  tokenizer=tokenizer,\n",
    "                  data_dir='/mnt/llm/data/toyreverse',\n",
    "                  output_dir='/mnt/llm_toyreverse')\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a965f08",
   "metadata": {},
   "source": [
    "## 4. run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77b46d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/llm_toyreverse/step=13616/weights.pt\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "torch.set_default_device('cuda')\n",
    "\n",
    "model = LLM(tokenizer.vocab_size(), tokenizer.pad_id(), Config())\n",
    "\n",
    "output_dir = '/mnt/llm_toyreverse'\n",
    "newest = max([f for f in os.listdir(output_dir)],\n",
    "             key=lambda x: os.path.getctime(os.path.join(output_dir, x)))\n",
    "checkpoint_path = os.path.join(output_dir, newest, 'weights.pt')\n",
    "print(checkpoint_path)\n",
    "\n",
    "state = torch.load(checkpoint_path, map_location='cuda:0')\n",
    "model.load_state_dict(state, strict=False)\n",
    "\n",
    "print(next(model.parameters()).is_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90c73f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt lUcxrqfpGQ5kzBcHrL773lEum4hcfG\n",
      "truth  Gfch4muEl377LrHcBzk5QGpfqrxcUl\n",
      "result fccmumEl33LLrccBcG5QffpGGxccc\n"
     ]
    }
   ],
   "source": [
    "prompt = generate_line()\n",
    "while len(tokenizer.encode(prompt)) >= Config().context_size:\n",
    "    prompt = prompt[:-2]\n",
    "print('prompt', prompt)\n",
    "print('truth ', prompt[::-1])\n",
    "\n",
    "x = torch.LongTensor(tokenizer.encode(prompt))\n",
    "pad = (0, Config().context_size - x.shape[0])\n",
    "x = F.pad(x, pad, \"constant\", tokenizer.pad_id())\n",
    "\n",
    "with torch.inference_mode():\n",
    "    logits = model.forward(x.unsqueeze(0).cuda())\n",
    "    probs = F.softmax(logits[0], dim=-1)\n",
    "    y = torch.argmax(probs, dim=-1)\n",
    "    result = tokenizer.decode(y.tolist())\n",
    "    print('result', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c0b590",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
